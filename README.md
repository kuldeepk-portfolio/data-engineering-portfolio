# Data Engineering Portfolio

Welcome to my Data Engineering Portfolio. Iâ€™m **Kuldeep Kumar**, a data-focused engineer with a strong background in SQL, Python, cloud infrastructure, and automation. This repository showcases a curated collection of end-to-end data engineering projects designed to demonstrate my expertise in building scalable, production-ready data pipelines across various technologies and platforms.

Each project is structured to reflect real-world data engineering scenarios, including data ingestion, transformation, storage, orchestration, and deployment in cloud environments.

---

## ðŸš€ Key Skills Demonstrated

- Batch and streaming data pipelines
- Data modeling (relational + NoSQL)
- Cloud-native architecture (AWS)
- ETL/ELT with Python, Spark, Pandas
- Data orchestration with Apache Airflow
- Real-time processing using Apache Kafka
- Infrastructure automation with Terraform & Shell

---

## ðŸ“ Project Index

| Project Name | Tech Stack | Description |
|--------------|------------|-------------|
| [`pandas-postgres-etl`](./pandas-postgres-etl) | Python, Pandas, PostgreSQL | Ingest, clean, and load customer data into a relational database |
| [`spark-nyc-taxi-pipeline`](./spark-nyc-taxi-pipeline) | PySpark, AWS S3 | Batch ETL pipeline on large-scale NYC Taxi dataset |
| [`kafka-log-streaming`](./kafka-log-streaming) | Kafka, Spark Structured Streaming | Real-time streaming and processing of application logs |
| [`airflow-sales-dag`](./airflow-sales-dag) | Apache Airflow, S3, Redshift | Orchestration of daily sales ETL pipeline |
| [`aws-glue-lakehouse`](./aws-glue-lakehouse) | AWS Glue, Athena, Redshift | Build a serverless lakehouse architecture on AWS |

---

## ðŸ§° Tech Stack

- **Languages**: Python, SQL, Shell
- **Processing**: Pandas, PySpark, Spark Streaming
- **Workflow Tools**: Apache Airflow
- **Cloud Platforms**: AWS (S3, Glue, Redshift, Athena)
- **Databases**: PostgreSQL, MongoDB, Redshift
- **Streaming**: Apache Kafka
- **Infrastructure**: Terraform, Git, Docker

---

## ðŸ“¦ How to Use

Each project is self-contained with:
- `README.md`: Documentation and instructions
- `scripts/` or `notebooks/`: Codebase
- `data/`: Sample or source data
- `requirements.txt`: Dependencies (if applicable)

Clone the repo and navigate into the project folder of interest:

```bash
git clone https://github.com/kuldeepk/data-engineering-portfolio.git
cd data-engineering-portfolio/pandas-postgres-etl

---
## ðŸ“ Notes

- All projects in this portfolio are developed for **educational and demonstrative purposes**.
- Data used in the pipelines is either **synthetic**, **open-source**, or **anonymized** to avoid any proprietary exposure.
- These projects are designed to showcase core skills in **data ingestion**, **processing**, **storage**, **orchestration**, and **cloud deployment**.
- Projects are intended to reflect **real-world workflows**, but may simplify or mock certain components to remain platform-neutral and lightweight.
- Feel free to fork or reuse any project structure, but review configs and scripts before applying to production environments.

---
## ðŸ™‹â€â™‚ï¸ About Me

I'm **Kuldeep Kumar**, a Principal System Analyst with over 19 years of experience in system automation, scripting, and data-focused application development.

Iâ€™m currently **transitioning into a cloud-native Data Engineering role**, blending my background in SQL, PL/SQL, automation, and cloud infrastructure with modern data engineering tools.

This portfolio reflects my learning and growth across:
- Batch and streaming data pipelines
- Data modeling (relational + NoSQL)
- Python-based data processing (Pandas, PySpark)
- Cloud data services (AWS S3, Glue, Redshift, Athena)
- Orchestration with Apache Airflow and infrastructure with Terraform

> My goal is to join top-tier companies like Google or Microsoft in a **Senior or Principal Data Engineer role** within the next 6â€“9 months.

- ðŸ”— [LinkedIn](https://www.linkedin.com/in/kuldeepkumar)
- ðŸ“« Email: [kuldeepk.engineer@gmail.com](mailto:kuldeepk.engineer@gmail.com)

